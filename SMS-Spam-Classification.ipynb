{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "797475c7",
   "metadata": {},
   "source": [
    "# Sms Spam-Classification Project using Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a010e0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9cf3d1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>messages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                           messages\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data= pd.read_csv(\"D:\\Study\\Data Science\\Dataset\\SMSSpamCollection.csv\",sep = \"\\t\", header= None, names = [\"label\", \"Messages\"],index_col= 0 )\n",
    "data= pd.read_csv(\"D:\\Study\\Data Science\\Dataset\\SMSSpamCollection.csv\",sep = \"\\t\", header= None, names = [\"label\", \"messages\"])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9836b58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.messages[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70b70031",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4bd9d3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Rofl. Its true to its name'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.messages[5571]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e10acccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk is a suite of libraries and programs for symbolic and statistical natural language processing for English\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0650c2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular expression to predict or analyze the text\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3e7d5a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Ravi0dubey\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stopwords are the English words which does not add much meaning to a sentence.\n",
    "# They can safely be ignored without sacrificing the meaning of the sentence. \n",
    "# For example, the words like the, he, have etc.\n",
    "# Such words are already captured this in corpus named corpus.\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45a90a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7deb021",
   "metadata": {},
   "source": [
    "# Difference between Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcba2a12",
   "metadata": {},
   "source": [
    "### Stemming \n",
    " 1. It is the process of removing the last few characters of a given word, to obtain a shorter form also called based word, even if that form doesn’t  have any meaning. eg: It will replace words “history” and “historical” with base word “histori”. \n",
    " 2.  Stemming is used to get that base word. In cases such as sentiment analysis, spam classification, restaurant reviews etc., getting base word is important to know whether the word is positive or negative. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f8bec4",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "1. It overcomes the drawbacks of stemming,base word does not have any meaning, whereas lemmatization gives meaningful word.\n",
    "\n",
    "2. It takes more time then stemming because it finds meaningful word/ representation. Stemming just needs to get a base word and therefore takes less time.\n",
    "\n",
    "3. Stemming has its application in Sentiment Analysis while Lemmatization has its application in Chatbots, human-answering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47048aa",
   "metadata": {},
   "source": [
    "### Preprocessing of Message(X variable) using Stemming Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a36b168c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7c780c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PorterStemmer>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    "ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "308ef04b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5572"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43cd94a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming  is used to get root word of different variety of the words\n",
    "#i.e. likely, liked, likes -> root word is like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51f3064b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocessing of the input data\n",
    "\n",
    "corpus = []\n",
    "for i in range(0,len(data)):\n",
    "    # It will remove any words in the message wich does not belong to a-z or A-Z\n",
    "    review= re.sub('[^a-zA-Z]',' ' , data['messages'][i])   \n",
    "    # It will convert case of the message to lower\n",
    "    review_lower= review.lower()\n",
    "    # It will split entire messages into individual words\n",
    "    review_split = review_lower.split()\n",
    "    # it will remove all the words from the messages which are part of stopwords\n",
    "    review_split_stem= [ps.stem(word) for word in review_split if not word in stopwords.words('english')]\n",
    "    # words will be joined within ' '\n",
    "    review_final = ' '.join(review_split_stem)\n",
    "    corpus.append(review_final)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795b3d09",
   "metadata": {},
   "source": [
    "### Printing output of each steps peformed above to showcase what happened in background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d26060c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Rofl  Its true to its name'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Review holds all words but except the ones which does not have valid alphabets in it\n",
    "\n",
    "review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "06ffba35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rofl  its true to its name'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# review_lower will store the lowercase of the the sentence\n",
    "review_lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "176425f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rofl', 'its', 'true', 'to', 'its', 'name']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# review_split will store the individual words of the sentences which are divided/splitted by spaces\n",
    "review_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e40f028e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rofl', 'true', 'name']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# review_split_stem stores words which do not belongs to stopwords \n",
    "\n",
    "review_split_stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "72393e7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rofl true name'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# review_final will store the sentence formed after above all process performed on the words\n",
    "review_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1ce4ae",
   "metadata": {},
   "source": [
    "### After data cleanup is done, countvectorizer is called to convert the message(X variable) into vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7898217b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv= CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2b8dbf94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X= cv.fit_transform(corpus).toarray()\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b61f4cb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 6296)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173e8fdf",
   "metadata": {},
   "source": [
    "### Explanation of Bag of Words and how Vector gets created for the message(X variable) using Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eeb40245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let say we have 4 text\n",
    "\n",
    "# Text1 = Burger is delicious and expensive\n",
    "# Text2 = Burger is not delicious and expensive\n",
    "# Text3 = Burger is okay and cheap\n",
    "# Text4 = Burger is okay and burger is not cheap\n",
    "\n",
    "# Step 1 : Create bag of words which are part of Text1 to 4 \n",
    "# Bagofwords = [Burger, delicious, expensive, not, okay, cheap]\n",
    "\n",
    "# Now check what all text from Bagwords list belongs to each text(From 1 to 4)and place them in the array\n",
    "# if word is present mark it 1 and if not then mark it 0\n",
    "# if word is present more than once then count the number of occurence and place it in the vector\n",
    "\n",
    "# Text1 Vector = [1,1,1,0,0,0]\n",
    "# Text2 Vector = [1,1,1,1,0,0]\n",
    "# Text3 Vector = [1,0,0,0,1,1]\n",
    "# Text4 Vector = [2,0,0,1,1,1] -> Burger occured twice hence the count is 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f06b2e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Count Vector with maximum features of 2500\n",
    "\n",
    "cv= CountVectorizer(max_features= 2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "48b9efd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X= cv.fit_transform(corpus).toarray()\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "15d7fbb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 2500)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04d4eff",
   "metadata": {},
   "source": [
    "### Converting label(y_variable) using one hot encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b60390f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      spam\n",
       "0        0\n",
       "1        0\n",
       "2        1\n",
       "3        0\n",
       "4        0\n",
       "...    ...\n",
       "5567     1\n",
       "5568     0\n",
       "5569     0\n",
       "5570     0\n",
       "5571     0\n",
       "\n",
       "[5572 rows x 1 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y= pd.get_dummies(data['label'], drop_first= True)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f996a9a4",
   "metadata": {},
   "source": [
    "### Splitting of the dataset into train and test column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "43b8efcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "30e7014d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test,y_train, y_test= train_test_split(X,y,test_size= 0.25,random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "db5adc6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed591c6",
   "metadata": {},
   "source": [
    "### Gaussian Naive Bayes is used for model training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6568aea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "12bdb9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model= GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9812b51a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ravi0dubey\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ac831f7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 1], dtype=uint8)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict = model.predict(X_test)\n",
    "y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "495b3760",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "aaaace43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8628858578607322"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28625167",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes is used for model training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "446b0361",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ravi0dubey\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8628858578607322"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "model2= MultinomialNB()\n",
    "model2.fit(X_train,y_train)\n",
    "y_predict1 = model.predict(X_test)\n",
    "accuracy_score(y_test,y_predict1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3234dce3",
   "metadata": {},
   "source": [
    "### Prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c0f79083",
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_msg = \"common you are not in good mood\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "35febc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus1 = []\n",
    "# It will remove any words in the message wich does not belong to a-z or A-Z\n",
    "review= re.sub('[^a-zA-Z]',' ' , Test_msg)   \n",
    "# It will convert case of the message to lower\n",
    "review_lower= review.lower()\n",
    "# It will split entire messages into individual words\n",
    "review_split = review_lower.split()\n",
    "# it will remove all the words from the messages which are part of stopwords\n",
    "review_split_stem= [ps.stem(word) for word in review_split if not word in stopwords.words('english')]\n",
    "# words will be joined within ' '\n",
    "review_final = ' '.join(review_split_stem)\n",
    "corpus1.append(review_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "053eadcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['common good mood']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "65d98137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1= cv.fit_transform(corpus1).toarray()\n",
    "X1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cbd6fe05",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (1,3) (2500,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14204/3941103738.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my_predict1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0my_predict1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_X\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m         \u001b[0mjll\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_joint_log_likelihood\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjll\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py\u001b[0m in \u001b[0;36m_joint_log_likelihood\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    451\u001b[0m             \u001b[0mjointi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclass_prior_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    452\u001b[0m             \u001b[0mn_ij\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m0.5\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2.\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpi\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msigma_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 453\u001b[1;33m             n_ij -= 0.5 * np.sum(((X - self.theta_[i, :]) ** 2) /\n\u001b[0m\u001b[0;32m    454\u001b[0m                                  (self.sigma_[i, :]), 1)\n\u001b[0;32m    455\u001b[0m             \u001b[0mjoint_log_likelihood\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjointi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mn_ij\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (1,3) (2500,) "
     ]
    }
   ],
   "source": [
    "y_predict1 = model.predict(X1)\n",
    "y_predict1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd00835c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
